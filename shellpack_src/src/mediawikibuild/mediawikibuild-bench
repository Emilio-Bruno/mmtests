#!/bin/bash
###SHELLPACK preamble mediawikibuild-bench 1.18.6
MIRROR_LOCATION="$WEBROOT/mediawiki/"

MARIADB_VERSION=10.3.15
MARIADB_DBNAME=my_wiki
MARIADB_ADMIN_USER=root
MARIADB_ADMIN_PASSWORD=mmtests-default
MYSQLCMD="${SHELLPACK_SOURCES}/mariadbbuild-${MARIADB_VERSION}-installed/bin/mysql -u $MARIADB_ADMIN_USER -p$MARIADB_ADMIN_PASSWORD"
MYSQLADMIN="${SHELLPACK_SOURCES}/mariadbbuild-${MARIADB_VERSION}-installed/bin/mysqladmin -u $MARIADB_ADMIN_USER -p$MARIADB_ADMIN_PASSWORD"

###SHELLPACK parseargBegin
###SHELLPACK parseargInstall
###SHELLPACK parseargYes	--start		MEDIAWIKI_START
###SHELLPACK parseargYes	--stop		MEDIAWIKI_STOP
###SHELLPACK parseargYes	--init		MEDIAWIKI_INIT
###SHELLPACK parseargEnd

###SHELLPACK check_install_required_continue mediawikibuild-${VERSION}

# see install script
BUFF_POOL_SIZE=$(($MEMTOTAL_BYTES/2))
DBSTART_OPTIONS="--innodb_flush_method=nosync,--innodb_flush_log_at_trx_commit=0,--innodb_buffer_pool_size=${BUFF_POOL_SIZE},--innodb_log_file_size=512M,--max_allowed_packet=1G"

function mediawiki_start() {
    $SHELLPACK_INCLUDE/shellpack-bench-mariadbbuild --stop
    echo Starting mariadb server...
    $SHELLPACK_INCLUDE/shellpack-bench-mariadbbuild --start \
	--start_opts $DBSTART_OPTIONS \
	--effective_cachesize $((MEMTOTAL_BYTES*6/10)) \
	--shared_buffers $((MEMTOTAL_BYTES/4)) \
	--work_mem $((16*1048576)) || die Failed to start mariadb server.

    $SHELLPACK_INCLUDE/shellpack-bench-apachebuild --stop
    echo Starting apache server...
    $SHELLPACK_INCLUDE/shellpack-bench-apachebuild --start || die Failed to start apache http server.
}

function mediawiki_stop() {
    echo Stopping apache server...
    $SHELLPACK_INCLUDE/shellpack-bench-apachebuild --stop
    echo Stopping mariadb server...
    $SHELLPACK_INCLUDE/shellpack-bench-mariadbbuild --stop
}

function mediawiki_init() {
    MWDUMPER_VERSION=1.16
    MWDUMPER_JAR=mwdumper.jar
    # see comments for chosen date of dumps.
    DUMP_YEAR=20080103
    # this will generate a little over 6.2 million pages
    DUMP_FILE_XML=enwiki-$DUMP_YEAR-pages-articles.xml

    # Uncomment the below to replace the dump used. Unfortunately these
    # are incompatible with wmdumper -- see comments above.
    # Also, this dump file is considerably larger and can take days to
    # dump (over 16 million pages).
#    DUMP_FILE_XML=enwiki-latest-pages-articles-multistream.xml
#    wget -c http://dumps.wikimedia.org/enwiki/latest/$DUMP_FILE_XML.bz2

    WEB_LOCATION=http://dumps.wikimedia.org/tools
    sources_fetch $WEB_LOCATION/mwdumper.jar $MIRROR_LOCATION/mwdumper.jar mwdumper.jar

    WEB_LOCATION=https://archive.org/download/enwiki-$DUMP_YEAR
    sources_fetch $WEB_LOCATION/$DUMP_FILE_XML.bz2 $MIRROR_LOCATION/$DUMP_FILE_XML.bz2 $DUMP_FILE_XML.bz2

    mmtests_activity database-init
    echo uncompressing ...
    eval bunzip2 $DUMP_FILE_XML.bz2 || die Failed to setup mediawiki

    # Some java flavors' --server option will increase the performance
    # and in a huge way for Sun's JVM large files (such as $DUMP_FILE_XML).
    #
    echo Importing mwdump, this will take a while...
    $TIME_CMD -o $LOGDIR_RESULTS/time-import-mwdump \
	java -jar $MWDUMPER_JAR --format=sql:1.5 $DUMP_FILE_XML 2> $LOGDIR_RESULTS/import-mwdump.log | \
		$MYSQLCMD --force $MARIADB_DBNAME 2>&1 | tee $LOGDIR_RESULTS/import-mwdump-mysql.log
    rm $DUMP_FILE_XML

    #
    # Download additional dumps that fill tables other than the main articles,
    # as it will allow us to service more requests, instead of throwing 404s.
    #
    # image.sql: Metadata on current versions of uploaded images.
    # imagelinks.sql: Wiki image usage records.
    # pagelinks.sql: Wiki page-to-page link records.
    # externallinks.sql: Wiki external URL link records.
    # categorylinks.sql: Wiki category membership link records.
    # logging.sql: Data for various events (deletions, uploads, etc).
    #
    # These additions will obviously enlarge the required storage
    # capabilities (see sizeof each dump for reference), so commenting
    # one or more of these is perfectly ok.
    #
    addons=( image.sql \
	imagelinks.sql \
	pagelinks.sql \
	externallinks.sql \
	categorylinks.sql \
	logging.sql )

    for addon in "${addons[@]}"
    do
	ADDON_FILE=enwiki-$DUMP_YEAR-$addon

	WEB_LOCATION=https://archive.org/download/enwiki-$DUMP_YEAR
	sources_fetch $WEB_LOCATION/$ADDON_FILE.gz $MIRROR_LOCATION/$ADDON_FILE.gz $ADDON_FILE.gz
	gunzip $ADDON_FILE.gz

	# Each of these downloaded .sql files create their corresponding tables
	# using the TYPE declaration, which is deprecated and breaks the entire
	# script... use ENGINE instead.
	sed -i -e "s/TYPE=InnoDB/ENGINE=InnoDb/" $ADDON_FILE

	echo Importing $addon dump ...
	ADDON_NAME=`echo $addon | sed -e 's/.sql//'`
	$TIME_CMD -o $LOGDIR_RESULTS/time-import-$ADDON_NAME \
		$MYSQLCMD --force $MARIADB_DBNAME < $ADDON_FILE 2>&1 | tee $LOGDIR_RESULTS/import-${ADDON_NAME}.log
	rm $ADDON_FILE
    done

    echo Completed wikipedia database dump.
}

if [ "$MEDIAWIKI_START" = "yes" ]; then
	mediawiki_start
fi

if [ "$MEDIAWIKI_STOP" = "yes" ]; then
	mediawiki_stop
fi

if [ "$MEDIAWIKI_INIT" = "yes" ]; then
	mediawiki_start
	mediawiki_init
	mediawiki_stop
fi

echo mediawikibuild successfully installed
exit $SHELLPACK_SUCCESS
